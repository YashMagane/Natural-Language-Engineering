{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YashMagane/Natural-Language-Engineering/blob/main/NLassignment2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2S8I2ny-ovS"
      },
      "source": [
        "# NLE Assignment: Sentiment Classification\n",
        "\n",
        "In this assignment, you will be investigating NLP methods for distinguishing positive and negative reviews written about movies.\n",
        "\n",
        "For assessment, you are expected to complete and submit this notebook file.  When answers require code, you may import and use library functions (unless explicitly told otherwise).  All of your own code should be included in the notebook rather than imported from elsewhere.  Written answers should also be included in the notebook.  You should insert as many extra cells as you want and change the type between code and markdown as appropriate.\n",
        "\n",
        "In order to avoid misconduct, you should not talk about the assignment questions with your peers.  If you are not sure what a question is asking you to do or have any other questions, please ask me or one of the Teaching Assistants.\n",
        "\n",
        "Marking guidelines are provided as a separate document.\n",
        "\n",
        "The first few cells contain code to set-up the assignment and bring in some data.   In order to provide unique datasets for analysis by different students, you must enter your candidate number in the following cell.  Otherwise do not change the code in these cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1gXQAZas-l9c"
      },
      "outputs": [],
      "source": [
        "candidateno=249763 #this MUST be updated to your candidate number so that you get a unique data sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nk8JTP88A8vs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9aacc25-6140-4d17-cc24-9e80a0b32dfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        }
      ],
      "source": [
        "#do not change the code in this cell\n",
        "#preliminary imports\n",
        "\n",
        "#set up nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('movie_reviews')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import movie_reviews\n",
        "\n",
        "#for setting up training and testing data\n",
        "import random\n",
        "\n",
        "#useful other tools\n",
        "import re\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from itertools import zip_longest\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.classify.api import ClassifierI\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BHBkzAccCVaZ"
      },
      "outputs": [],
      "source": [
        "#do not change the code in this cell\n",
        "def split_data(data, ratio=0.7): # when the second argument is not given, it defaults to 0.7\n",
        "    \"\"\"\n",
        "    Given corpus generator and ratio:\n",
        "     - partitions the corpus into training data and test data, where the proportion in train is ratio,\n",
        "\n",
        "    :param data: A corpus generator.\n",
        "    :param ratio: The proportion of training documents (default 0.7)\n",
        "    :return: a pair (tuple) of lists where the first element of the\n",
        "            pair is a list of the training data and the second is a list of the test data.\n",
        "    \"\"\"\n",
        "\n",
        "    data = list(data)\n",
        "    n = len(data)\n",
        "    train_indices = random.sample(range(n), int(n * ratio))\n",
        "    test_indices = list(set(range(n)) - set(train_indices))\n",
        "    train = [data[i] for i in train_indices]\n",
        "    test = [data[i] for i in test_indices]\n",
        "    return (train, test)\n",
        "\n",
        "\n",
        "def get_train_test_data():\n",
        "\n",
        "    #get ids of positive and negative movie reviews\n",
        "    pos_review_ids=movie_reviews.fileids('pos')\n",
        "    neg_review_ids=movie_reviews.fileids('neg')\n",
        "\n",
        "    #split positive and negative data into training and testing sets\n",
        "    pos_train_ids, pos_test_ids = split_data(pos_review_ids)\n",
        "    neg_train_ids, neg_test_ids = split_data(neg_review_ids)\n",
        "    #add labels to the data and concatenate\n",
        "    training = [(movie_reviews.words(f),'pos') for f in pos_train_ids]+[(movie_reviews.words(f),'neg') for f in neg_train_ids]\n",
        "    testing = [(movie_reviews.words(f),'pos') for f in pos_test_ids]+[(movie_reviews.words(f),'neg') for f in neg_test_ids]\n",
        "\n",
        "    return training, testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N3LWwBYICPP"
      },
      "source": [
        "When you have run the cell below, your unique training and testing samples will be stored in `training_data` and `testing_data`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HJLegkdPFUJA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cac3c557-9156-4f0e-a870-4ad64fa2c13e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The amount of training data is 1400\n",
            "The amount of testing data is 600\n",
            "The representation of a single data item is below\n",
            "(['perhaps', 'the', 'most', 'dramatic', 'changes', 'in', ...], 'pos')\n"
          ]
        }
      ],
      "source": [
        "#do not change the code in this cell\n",
        "random.seed(249763)\n",
        "training_data,testing_data=get_train_test_data()\n",
        "print(\"The amount of training data is {}\".format(len(training_data)))\n",
        "print(\"The amount of testing data is {}\".format(len(testing_data)))\n",
        "print(\"The representation of a single data item is below\")\n",
        "print(training_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE3bKQbB50Rq"
      },
      "source": [
        "1)  \n",
        "a) **Generate** a list of 10 content words which are representative of the positive reviews in your training data.\n",
        "\n",
        "b) **Generate** a list of 10 content words which are representative of the negative reviews in your training data.\n",
        "\n",
        "c) **Explain** what you have done and why\n",
        "\n",
        "[20\\%]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QkT9CWv250Rq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "faa08fd8-889e-4821-b93e-8bf53995762b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopping = stopwords.words('english')\n",
        "\n",
        "def list_norm(data, tag):\n",
        "    if tag != '':\n",
        "        words_normalisation = [([words for words in i[0] if words.isalpha() and words not in stopping], i[1]) for i in data if i[1] == tag]\n",
        "        bag_of_words = [(FreqDist(i[0]), i[1]) for i in words_normalisation]\n",
        "    else:\n",
        "        words_normalisation = [([words for words in i[0] if words.isalpha() and words not in stopping], i[1]) for i in data]\n",
        "        bag_of_words = [(FreqDist(i[0]), i[1]) for i in words_normalisation]\n",
        "    return bag_of_words\n",
        "\n",
        "def most_frequent_words(freq_dist_one, freq_dist_two, top_of_pos_or_neg):\n",
        "    difference = freq_dist_one - freq_dist_two\n",
        "    return list(dict(difference.most_common(top_of_pos_or_neg)).keys())\n",
        "\n",
        "def freq_dist(data):\n",
        "    my_freq_dist = FreqDist()\n",
        "    for review, label in data:\n",
        "        my_freq_dist += review\n",
        "    return my_freq_dist\n",
        "\n",
        "training_pos_freq = list_norm(training_data, \"pos\")\n",
        "training_neg_freq = list_norm(training_data, \"neg\")\n",
        "\n",
        "pos_freq_dist = freq_dist(training_pos_freq)\n",
        "neg_freq_dist = freq_dist(training_neg_freq)\n",
        "\n",
        "bag_of_training_pos = most_frequent_words(pos_freq_dist, neg_freq_dist, 10)\n",
        "bag_of_training_neg = most_frequent_words(neg_freq_dist, pos_freq_dist, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GQBi7JVR50Rr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42c3d8f5-d58b-498e-9205-5ffa3bc5f02d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['film', 'life', 'also', 'well', 'best', 'great', 'world', 'films', 'many', 'story']\n"
          ]
        }
      ],
      "source": [
        "print(bag_of_training_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OIqSAumd50Rs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4c9f41b-50bf-4c48-a71b-4415a491ef08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['bad', 'movie', 'plot', 'even', 'worst', 'stupid', 'like', 'could', 'nothing', 'boring']\n"
          ]
        }
      ],
      "source": [
        "print(bag_of_training_neg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zljze_ZJ50Rt"
      },
      "source": [
        "When given a corpus, the computer sees a sequence of characters, reprensented as a sequence of binary numbers. This makes it difficult to efficiently search through a collection of documents which just characters. Therefore, for retrieval of documents we need to preprocess the documents in order for efficient search. This is why I first began my code by removing any stopwords, these are words in which are very common and don't carry a massive amount of information, they are ignored by the NLTK tokenisers. My next steps were about normalising the list given, this gets rid of punctuation marks and lowercasing all characters. All of this allows efficient search to be made on the collection of documents. I then created functions which make a frequency distribution of the data that has been split into postive and negative reviews. This is so we can use the frequency distribution to observe and calculate properties of the distribution, allowing us to make a decision. Finally, I created two different bag of words which have postive words in one and negative in the other, so I could print them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TApOQE6vND20"
      },
      "source": [
        "2)\n",
        "a) **Use** the lists generated in Q1 to build a **word list classifier** which will classify reviews as being positive or negative.\n",
        "\n",
        "b) **Explain** what you have done.\n",
        "\n",
        "[12.5\\%]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BThDMrcmODJy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "f0f710f9-74ea-4b69-fea4-1ec6603a2031"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'neg'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from nltk.classify.api import ClassifierI\n",
        "\n",
        "class WordsListClassifier(ClassifierI):\n",
        "\n",
        "    def __init__(self, pos, neg):\n",
        "        self._pos = pos\n",
        "        self._neg = neg\n",
        "\n",
        "    def classify(self, words):\n",
        "        score = 0\n",
        "        for word, value in words.items():\n",
        "            if word in self._pos:\n",
        "                score +=value\n",
        "            if word in self._neg:\n",
        "                score -=value\n",
        "        if score > 0:\n",
        "            return 'pos'\n",
        "        elif score == 0 or score < 0:\n",
        "          return 'neg'\n",
        "\n",
        "    def labels(self):\n",
        "        return (\"pos\", \"neg\")\n",
        "\n",
        "classifier = WordsListClassifier(bag_of_training_pos, bag_of_training_neg)\n",
        "classifier.classify(FreqDist(\"terrible movie\".split()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6xxvcyr50Rv"
      },
      "source": [
        "For this question I made my very own classifier, which takes words splits them and tries to work out whether they are positive words or negatives, the end result is a statement neg or pos for negative and positive. This is done by using the variable score, it takes away one everytime a negative word is detected or adds one if a positive word is detected. If the final score is positive the whole review is positive and if the final score is negative the whole review is negative. At the end I tried using a string to check if my classifier is working correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL1iL9jg50Rv"
      },
      "source": [
        "3)\n",
        "a) **Calculate** the accuracy, precision, recall and F1 score of your classifier.\n",
        "\n",
        "b) Is it reasonable to evaluate the classifier in terms of its accuracy?  **Explain** your answer and give a counter-example (a scenario where it would / would not be reasonable to evaluate the classifier in terms of its accuracy).\n",
        "\n",
        "[20\\%]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "r1PrZnTe50Rw"
      },
      "outputs": [],
      "source": [
        "class ConfusionMatrix:\n",
        "    def __init__(self,predictions,goldstandard,classes=(\"pos\",\"neg\"), display = True):\n",
        "\n",
        "        (self.c1,self.c2)=classes\n",
        "        self.TP=0\n",
        "        self.FP=0\n",
        "        self.FN=0\n",
        "        self.TN=0\n",
        "        for p,g in zip(predictions,goldstandard):\n",
        "            if g==self.c1:\n",
        "                if p==self.c1:\n",
        "                    self.TP+=1\n",
        "                else:\n",
        "                    self.FN+=1\n",
        "\n",
        "            elif p==self.c1:\n",
        "                self.FP+=1\n",
        "            else:\n",
        "                self.TN+=1\n",
        "        if display == True:\n",
        "            print(f'True positive (tp) : {self.TP}\\nFalse positive (fp) : {self.FP}\\nTrue Negative (tn) : {self.TN}\\nFalse negative (fn) : {self.FN}')\n",
        "\n",
        "    def accuracy(self):\n",
        "        self.acc = 0\n",
        "        self.acc = (self.TP+self.TN)/(self.TP+self.FP+self.FN+self.TN)\n",
        "        return self.acc\n",
        "\n",
        "    def precision(self):\n",
        "        self.p=0\n",
        "        self.p = self.TP/(self.TP+self.FP)\n",
        "        return self.p\n",
        "\n",
        "    def recall(self):\n",
        "        self.r=0\n",
        "        self.r = self.TP/(self.TP+self.FN)\n",
        "        return self.r\n",
        "\n",
        "    def f1_score(self):\n",
        "        self.f1=0\n",
        "        self.f1 = 2*self.p*self.r/(self.p+self.r)\n",
        "        return self.f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Uq4Fmr8S50Rx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b9ca2ae-348b-46dc-ae09-0d900b2f83b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True positive (tp) : 223\n",
            "False positive (fp) : 131\n",
            "True Negative (tn) : 169\n",
            "False negative (fn) : 77\n"
          ]
        }
      ],
      "source": [
        "testing_norm = list_norm(testing_data, '')\n",
        "test_cases, labels = zip(*testing_norm)\n",
        "myClassifer = WordsListClassifier(bag_of_training_pos, bag_of_training_neg)\n",
        "prediction = myClassifer.classify_many(test_cases)\n",
        "myClassifer = ConfusionMatrix(prediction, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NJhRGovu50Ry",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "34de3d05-0e02-48f7-c073-0e1c5d84bbca"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   Accuracy  Precision    Recall   F1Score\n",
              "0  0.653333   0.629944  0.743333  0.681957"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3590392c-18e9-439b-ab04-57214dd35fbf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.653333</td>\n",
              "      <td>0.629944</td>\n",
              "      <td>0.743333</td>\n",
              "      <td>0.681957</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3590392c-18e9-439b-ab04-57214dd35fbf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3590392c-18e9-439b-ab04-57214dd35fbf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3590392c-18e9-439b-ab04-57214dd35fbf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_66cc9ac3-8671-4e08-98a2-892ccbb2f36e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('table')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_66cc9ac3-8671-4e08-98a2-892ccbb2f36e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('table');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "table",
              "summary": "{\n  \"name\": \"table\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"Accuracy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6533333333333333,\n        \"max\": 0.6533333333333333,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6533333333333333\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6299435028248588,\n        \"max\": 0.6299435028248588,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6299435028248588\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.7433333333333333,\n        \"max\": 0.7433333333333333,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.7433333333333333\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.6819571865443425,\n        \"max\": 0.6819571865443425,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6819571865443425\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "metrics_wordlist_classifier = {\"Accuracy\" : myClassifer.accuracy(), \"Precision\" :myClassifer.precision(), \"Recall\" : myClassifer.recall(), \"F1Score\" : myClassifer.f1_score()}\n",
        "table = pd.DataFrame(metrics_wordlist_classifier, index = [0])\n",
        "display(table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gHYwj0i50Ry"
      },
      "source": [
        "Accuracy is a measurement for the proportion classified items in the data. However, it is not the best metric to fully evaluate a classifier.This is because accuracy does not work well with imbalanced data, this is data that has higher number of items on one argument and fewer in the other. For instance, if the number of positive reviews were to be 70% of the data and negative reviews 30% and the classifier fails to predict the negative reviews, it's accuracy would still be 70%, but that's not representing the whole data. In bigger scenarios, accuracy will not be a good choice of evaluating a classifier as there is a very low likelihood of a data set being split in perfect half. But in the case that somehow your data is split 50%, accuracy would be a great measure of evaluation, as it will show a true value for your data, allowing you to make better decisions based on the value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIS9UpmJNEAp"
      },
      "source": [
        "4)\n",
        "a)  **Construct** a Naive Bayes classifier (e.g., from NLTK).\n",
        "\n",
        "b)  **Compare** the performance of your word list classifier with the Naive Bayes classifier.  **Discuss** your results.\n",
        "\n",
        "[12.5\\%]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hG4DSeqD50Rz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "outputId": "e4801680-511d-4b26-91cc-04b8fbd8bd0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True positive (tp) : 294\n",
            "False positive (fp) : 158\n",
            "True Negative (tn) : 142\n",
            "False negative (fn) : 6\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DataFrame' object has no attribute 'append'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-3cee4b236c58>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmatrix_for_NB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConfusionMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnaive_class_from_nltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify_many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_cases\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pos'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'neg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmetrics_NB_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"Accuracy\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mmatrix_for_NB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Precision\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mmatrix_for_NB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Recall\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mmatrix_for_NB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"F1 score\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mmatrix_for_NB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtable2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics_NB_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6202\u001b[0m         ):\n\u001b[1;32m   6203\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6204\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6206\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
          ]
        }
      ],
      "source": [
        "from nltk.classify import NaiveBayesClassifier\n",
        "naive_training_data = list_norm(training_data, '')\n",
        "naive_class_from_nltk = NaiveBayesClassifier.train(naive_training_data)\n",
        "matrix_for_NB = ConfusionMatrix(naive_class_from_nltk.classify_many(test_cases), labels ,classes=('pos','neg'))\n",
        "metrics_NB_classifier = {\"Accuracy\" : matrix_for_NB.accuracy(), \"Precision\" : matrix_for_NB.precision(), \"Recall\" : matrix_for_NB.recall(), \"F1 score\" : matrix_for_NB.f1_score()}\n",
        "table2 = table.append(metrics_NB_classifier, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56_oD8ET50Rz"
      },
      "outputs": [],
      "source": [
        "display(table2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfuYer9U50Rz"
      },
      "source": [
        "There is a difference in the values of all atributes for both classifiers. The Naive Bayes Classifer seems to be the better classifier than my word classifier on unseen documents as it produced higher precision and recall values. The higher recall means that the Naive Bayes Classifier predicted correctly a greater number of +ve documents in proportion than my classifier or in other words less false negaties were detected by the NB Classifier. The precision value shows the proportion of +ve predictions that are correct, meaning less false positives were detected by the NB classifier, making it better than my classifier. The Accuracy and F1 score of NB classifier is also higher, these are clear values that NB's Classifier is better as the accuracy shows proportion of items classified correctly. The F1 score shows the harmonic mean between precision and recall, Since NB's classifier has higher precision and recall than my classifier we can automatically infer that the F1 score for NB's classifier is higher, making it the better performing classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGDXaVDqOSfY"
      },
      "source": [
        "5)\n",
        "a) Design and **carry out an experiment** into the impact of the **length of the wordlists** on the wordlist classifier.  Make sure you **describe** design decisions in your experiment, include a **graph** of your results and **discuss** your conclusions.\n",
        "\n",
        "b) Would you **recommend** a wordlist classifier or a Naive Bayes classifier for future work in this area?  **Justify** your answer.\n",
        "\n",
        "[25\\%]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PVBBPhh50R0"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tBhqZtw50R1"
      },
      "outputs": [],
      "source": [
        "metrics = []\n",
        "for i in range(1, 10, 100):\n",
        "    bag_of_training_pos_ = most_frequent_words(pos_freq_dist, neg_freq_dist, i)\n",
        "    bag_of_training_neg_ = most_frequent_words(neg_freq_dist, pos_freq_dist, i)\n",
        "    bags_classified = WordsListClassifier(bag_of_training_pos_, bag_of_training_neg_)\n",
        "    predic = bags_classified.classify_many(test_cases)\n",
        "    graph = ConfusionMatrix(predic, labels, display = False)\n",
        "    metrics.append([i, (graph.accuracy(), graph.precision(), graph.recall(), graph.f1_score())])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1ozKYMa50R1"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize = (14, 10))\n",
        "ax[1, 0].plot([i[0] for i in metrics], [i[1][0] for i in metrics], c = 'blue')\n",
        "ax[1, 0].set_title('Impact of the length of the wordlists on the wordlist classifier')\n",
        "ax[1, 0].set(ylabel = 'Accuracy' ,xlabel = 'Length of Wordlist')\n",
        "ax[1, 0].set(ylabel = 'Precision' ,xlabel = 'Length of Wordlist')\n",
        "ax[1, 0].set(ylabel = 'Recall' ,xlabel = 'Length of Wordlist')\n",
        "ax[1, 0].set(ylabel = 'F1Score' ,xlabel = 'Length of Wordlist')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzN_HIuC50R1"
      },
      "outputs": [],
      "source": [
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYZVd15L50R2"
      },
      "source": [
        "Both Classifiers have their disadvantages and advantages. The wordlist classifier, does its job of classifying the data in a document, but it does not do it as accurately as Naive Bayes Classifier. The better the accuracy the better the decision making, however, as previously explored it is not a good metric to evaluate on a classifier due to imbalances in the data. A disadvantage of the NB classifier is that it assumes that all features are independent, which is not always the case in the real world, this makes it harder to use if there are some set of independent events. This can be disregarded as the other metric values that NB's classifier produces come with a lot of importance. It provides higher precision and recall values, making the overall F1 score higher, This allows me to make better decisions. Although I do not have a valid experiment to explain the impact of length of word lists for the word list classifier, I have learnt that, the accuracy, precision, recall and F1 score of the classifier would get lower over the length of wordlists increases, as data set increases, there is a higher chance of getting false postives and negatives not predicted. On the other hand, NB's classifier would perform better as there is less of an impact if the data set increases. In conclusion, I would be using a NB Classifier when working in this area in the future, as this classifier performs better."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "WMN_tUrBOqUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34rdlS_iPov6"
      },
      "outputs": [],
      "source": [
        "##This code will word count all of the markdown cells in the notebook saved at filepath\n",
        "##Running it before providing any answers shows that the questions have a word count of 437\n",
        "\n",
        "import io\n",
        "from nbformat import current\n",
        "\n",
        "filepath=\"/content/drive/My Drive/NLassignment2022.ipynb\"\n",
        "#filepath=\"NLassignment2021.ipynb\"\n",
        "question_count=437\n",
        "\n",
        "with io.open(filepath, 'r', encoding='utf-8') as f:\n",
        "    nb = current.read(f, 'json')\n",
        "\n",
        "word_count = 0\n",
        "for cell in nb.worksheets[0].cells:\n",
        "    if cell.cell_type == \"markdown\":\n",
        "        word_count += len(cell['source'].replace('#', '').lstrip().split(' '))\n",
        "print(\"Submission length is {}\".format(word_count-question_count))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}